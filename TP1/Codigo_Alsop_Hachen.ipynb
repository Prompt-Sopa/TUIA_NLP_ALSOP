{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV86mNULfArI"
      },
      "source": [
        "# Procesamiento del Lenguaje Natural - TUIA\n",
        "---\n",
        "## Trabajo Pr√°ctico 1 - Clasificador de Recomendaciones Recreativas utilizando NLP\n",
        "\n",
        "Integrantes:\n",
        "- Alsop Agust√≠n (A-4651/7)\n",
        "- Hachen Roc√≠o (H-1184/3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMj7eJYhM0HQ"
      },
      "source": [
        "### Instalaciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cfWhlARYM2PP",
        "outputId": "57fe2e91-301e-463c-df5f-680d6762b668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-lg==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.7.0/es_core_news_lg-3.7.0-py3-none-any.whl (568.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m821.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-lg==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_lg')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download es_core_news_lg\n",
        "!pip install sentence_transformers\n",
        "#!pip install thefuzz python-Levenshtein\n",
        "#!pip install transformers sentence_transformers\n",
        "#!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO5umZA5B-cz"
      },
      "source": [
        "Se recomienda reiniciar el entorno luego de las instalaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLmDWAGXfsm0"
      },
      "source": [
        "## Librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n46AqKzkoHK-"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "#import nltk\n",
        "#from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "#from thefuzz import process\n",
        "#import requests\n",
        "#from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "from google.colab import output\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "from datetime import datetime\n",
        "import pytz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtWTGOcQMTgE"
      },
      "source": [
        "## Funciones\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU8U31aYNIQO"
      },
      "source": [
        "### Identificaci√≥n de frases problem√°ticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tduUuMaLmzf"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"es_core_news_lg\")\n",
        "\n",
        "frases_peligrosas = [\n",
        "    \"suicidio\", \"suicidarse\", \"no quiero vivir\", \"quiero morir\", \"me voy a matar\",\n",
        "    \"pegarse un tiro\", \"matarme\", \"acabar con todo\", \"lastimarme\", \"me hago da√±o\",\n",
        "    \"me quiero cortar\", \"terminarlo todo\", \"me odio\", \"quiero desaparecer\",\n",
        "    \"no puedo seguir as√≠\", \"me quiero morir\", \"ojal√° desapareciera\",\n",
        "    \"me voy de este mundo\", \"no le importo a nadie\", \"no tengo a nadie\",\n",
        "    \"nadie se preocupa por m√≠\", \"ya no aguanto m√°s\", \"hasta aqu√≠ llegu√©\",\n",
        "    \"mi vida no vale nada\", \"ojal√° no despertara\", \"no tengo miedo a morir\",\n",
        "    \"un descanso eterno\", \"tirar por un puente\", \"me quiero tirar por un puente\"\n",
        "]\n",
        "\n",
        "def redFlagIdentifier(frase : str) -> bool:\n",
        "    \"\"\"\n",
        "    Verifica si una frase ingresada contiene indicios de pensamientos o expresiones relacionadas con autolesiones\n",
        "    \"\"\"\n",
        "\n",
        "    doc_frase = nlp(frase.lower())\n",
        "\n",
        "    # Verificamos la similitud con las frases peligrosas\n",
        "    for frase_peligrosa in frases_peligrosas:\n",
        "        doc_peligroso = nlp(frase_peligrosa)\n",
        "        if doc_frase.similarity(doc_peligroso) > 0.7:  # Umbral de similitud\n",
        "            return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQvTOeC45Lo5"
      },
      "source": [
        "#### Test de la funci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmmlujKZOZID",
        "outputId": "32035ab6-d13a-44c2-c719-fd5d961c0451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frase: 'me voy a suicidar' - ¬øPeligrosa?: True\n",
            "Frase: 'no quiero vivir m√°s' - ¬øPeligrosa?: True\n",
            "Frase: 'hoy es un mal d√≠a' - ¬øPeligrosa?: False\n",
            "Frase: 'no aguanto esto' - ¬øPeligrosa?: True\n",
            "Frase: 'me quiero tirar por un puente' - ¬øPeligrosa?: True\n",
            "Frase: 'la vida no tiene sentido' - ¬øPeligrosa?: False\n",
            "Frase: 'todos estar√≠an mejor sin m√≠' - ¬øPeligrosa?: True\n",
            "Frase: 'me siento vac√≠o' - ¬øPeligrosa?: True\n",
            "Frase: 'quiero acabar con todo' - ¬øPeligrosa?: True\n",
            "Frase: 'ojal√° desapareciera' - ¬øPeligrosa?: True\n",
            "Frase: 'me voy de este mundo' - ¬øPeligrosa?: True\n",
            "Frase: 'espero que esto termine pronto' - ¬øPeligrosa?: True\n",
            "Frase: 'me siento atrapado' - ¬øPeligrosa?: True\n",
            "Frase: 'la vida es demasiado dif√≠cil' - ¬øPeligrosa?: False\n",
            "Frase: 'no tengo a nadie que me quiera' - ¬øPeligrosa?: True\n",
            "Frase: 'voy a matar a alguien' - ¬øPeligrosa?: True\n"
          ]
        }
      ],
      "source": [
        "frases_polemicas = [\n",
        "    \"me voy a suicidar\",\n",
        "    \"no quiero vivir m√°s\",\n",
        "    \"hoy es un mal d√≠a\",\n",
        "    \"no aguanto esto\",\n",
        "    \"me quiero tirar por un puente\",\n",
        "    \"la vida no tiene sentido\",\n",
        "    \"todos estar√≠an mejor sin m√≠\",\n",
        "    \"me siento vac√≠o\",\n",
        "    \"quiero acabar con todo\",\n",
        "    \"ojal√° desapareciera\",\n",
        "    \"me voy de este mundo\",\n",
        "\n",
        "    \"espero que esto termine pronto\",\n",
        "    \"me siento atrapado\",\n",
        "    \"la vida es demasiado dif√≠cil\",\n",
        "    \"no tengo a nadie que me quiera\",\n",
        "    \"voy a matar a alguien\"\n",
        "]\n",
        "\n",
        "# Iterar sobre las frases pol√©micas y verificar si se consideran peligrosas\n",
        "for frase in frases_polemicas:\n",
        "    resultado = redFlagIdentifier(frase)\n",
        "    print(f\"Frase: '{frase}' - ¬øPeligrosa?: {resultado}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIFcg8MU5WkH"
      },
      "source": [
        "### Detecci√≥n de emociones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waGH1eHqtnnY"
      },
      "outputs": [],
      "source": [
        "def analisisEmocion(input_usuario : str, modeloLR, modeloST) -> str :\n",
        "  \"\"\"\n",
        "  Analiza el estado emocional del usuario con base en el texto ingresado,\n",
        "  utilizando un modelo de regresi√≥n log√≠stica (modeloLR) para clasificaci√≥n de emociones\n",
        "  y el modelo Sentence Transformer (modeloST) para transformar la frase en una representaci√≥n vectorial.\n",
        "  \"\"\"\n",
        "  new_phrases = [input_usuario]\n",
        "\n",
        "  # Preprocesamiento y vectorizaci√≥n de las nuevas frases\n",
        "  new_phrases_lower = [text.lower() for text in new_phrases]\n",
        "  new_phrases_vectorized = modeloST.encode(new_phrases_lower)\n",
        "\n",
        "  # Haciendo predicciones con el modelo entrenado\n",
        "  new_predictions = modeloLR.predict(new_phrases_vectorized)\n",
        "\n",
        "  # Definimos el mapeo para las etiquetas\n",
        "  labels = {-1: 'triste', 0: 'neutro', 1: 'feliz'}\n",
        "\n",
        "  # Mostrando las predicciones junto con las frases\n",
        "  for text, label in zip(new_phrases, new_predictions):\n",
        "      print(f\"Texto: '{text}'\")\n",
        "      print(f\"Clasificaci√≥n predicha: {labels[label]}\\n\")\n",
        "\n",
        "  return new_predictions[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewUpvxU150B9"
      },
      "source": [
        "### Funciones secundarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vuvQAaxctr0"
      },
      "outputs": [],
      "source": [
        "def yesNoValidator() -> bool:\n",
        "  \"\"\"\n",
        "  Valida respuestas de s√≠ o no ingresadas por el usuario.\n",
        "  Acepta varias variantes de cada respuesta (como \"Y\", \"Yes\", \"Si\", etc.), verificando que la entrada sea v√°lida antes de continuar.\n",
        "  \"\"\"\n",
        "\n",
        "  rta = input(\"Responda si o no (Y/N): \")\n",
        "  yes_answers = [\"Y\", \"y\", \"Yes\", \"Si\",\"S\",\"s\",\":)\"]\n",
        "  no_answers = [\"N\", \"n\", \"No\",\":(\"]\n",
        "  valid_answers = yes_answers + no_answers\n",
        "  while rta not in valid_answers:\n",
        "    print(f'Por favor conteste con las opciones v√°lidas: {valid_answers}')\n",
        "    rta = input(\"Responda si o no (Y/N): \")\n",
        "  if rta in yes_answers:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHc9czbARool"
      },
      "outputs": [],
      "source": [
        "def saludo() -> tuple:\n",
        "  \"\"\"\n",
        "  Genera un saludo contextualizado seg√∫n la hora local de Buenos Aires, Argentina,\n",
        "  devolviendo un mensaje de buenos d√≠as, buenas tardes o buenas noches, junto con la hora exacta en formato de 24 horas.\n",
        "  \"\"\"\n",
        "  local_timezone = pytz.timezone(\"America/Argentina/Buenos_Aires\")\n",
        "  hora_actual = datetime.now(local_timezone)\n",
        "\n",
        "  # Determina el saludo basado en la hora\n",
        "  if 6 <= hora_actual.hour < 12:\n",
        "      saludo = \"¬°Buenos d√≠as!\"\n",
        "  elif 12 <= hora_actual.hour < 20:\n",
        "      saludo = \"¬°Buenas tardes!\"\n",
        "  else:\n",
        "      saludo = \"¬°Buenas noches!\"\n",
        "\n",
        "  hora_minutos = hora_actual.strftime(\"%H:%M\")\n",
        "  return hora_minutos, saludo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui_vH1ZlOPub"
      },
      "outputs": [],
      "source": [
        "def loading_animation() -> None:\n",
        "  \"\"\"\n",
        "  Esta funci√≥n genera una animaci√≥n de carga en la consola utilizando una secuencia de emojis de gatos y puntos, simulando una espera mientras el sistema procesa alguna tarea.\n",
        "  \"\"\"\n",
        "  loading_text = \"Espere un momento\"\n",
        "  animation_frames = [\"üò∫\", \"üò∏\", \"üòπ\", \"üòª\", \"üòº\", \"üòΩ\", \"üôÄ\", \"üòø\", \"üòæ\"]\n",
        "  for _ in range(2):\n",
        "      punto = \".\"\n",
        "      for frame in animation_frames:\n",
        "          punto += \".\"\n",
        "          clear_output(wait=True)\n",
        "          print(f\"{loading_text}{punto} \\n{frame}\")\n",
        "          time.sleep(0.3)\n",
        "  print(\"¬°Carga completa!\")\n",
        "  time.sleep(0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8OXmZJUn_qq"
      },
      "outputs": [],
      "source": [
        "def activitySearch(prompt : str ,modelo ,db : pd.DataFrame ,quantity : int) -> None:\n",
        "  \"\"\"\n",
        "  Esta funci√≥n busca actividades en una base de datos seg√∫n la similitud sem√°ntica con el texto de entrada (prompt).\n",
        "  Utiliza generados por un modelo de transformers para identificar las actividades m√°s relevantes.\n",
        "  \"\"\"\n",
        "  input_embeddings = model2.encode(prompt, convert_to_tensor=True)\n",
        "\n",
        "  db['coincidencia_sbert'] = db['tensor'].apply(lambda x: util.cos_sim(x, input_embeddings).item())\n",
        "\n",
        "  db = db.sort_values(by='coincidencia_sbert', ascending=False)\n",
        "  top_3_activity = db[['Title', 'Description', 'coincidencia_sbert']].head(quantity)\n",
        "\n",
        "  print(f'B√∫squeda: {prompt}\\n')\n",
        "  print('Encontr√© estos resultados: ')\n",
        "  for index, row in top_3_activity.iterrows():\n",
        "    print(f\"T√≠tulo: {row['Title']}\")\n",
        "    print(f\"Descripci√≥n: {row['Description']}\")\n",
        "    print(f\"Similitud: {row['coincidencia_sbert']:.4f}\\n\")\n",
        "  #db[['Title','Description', 'coincidencia_sbert']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ7V0gfl59hr"
      },
      "source": [
        "### Menu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9haGmEPacja"
      },
      "outputs": [],
      "source": [
        "def respuestaEmocion(emocion_determinada : int, red_flag : bool = False  ) -> None:\n",
        "  \"\"\"\n",
        "  Maneja la respuesta del sistema seg√∫n la emoci√≥n determinada del usuario y si se detect√≥ alguna frase de riesgo (red_flag).\n",
        "  Bas√°ndose en la emoci√≥n del usuario, ofrece recomendaciones de actividades que pueden mejorar su estado de √°nimo.\n",
        "  \"\"\"\n",
        "  if emocion_determinada == 0:\n",
        "    print('Parece que el d√≠a se mantiene normal üòä. ¬øQuieres una recomendaci√≥n para el d√≠a de hoy?')\n",
        "    rta = yesNoValidator()\n",
        "    if rta:\n",
        "      output.clear()\n",
        "      print('¬°Genial! El d√≠a se presta para cualquiera de estas actividades, ¬øcu√°l prefieres?\\n Ingres√° el n√∫mero de la opci√≥n requerida')\n",
        "      print('1- Pel√≠culas\\n2- Juegos de mesa\\n3- Libros\\n4- Salir')\n",
        "      eleccion = input()\n",
        "      while eleccion not in ['1','2','3','4']:\n",
        "        output.clear()\n",
        "        print('Por favor ingrese una opci√≥n v√°lida')\n",
        "        print('¬øCu√°l prefieres?\\n Ingres√° el n√∫mero de la opci√≥n requerida üòä.')\n",
        "        print('1- Pel√≠culas\\n2- Juegos de mesa\\n3- Libros\\n4- Salir')\n",
        "        eleccion = input()\n",
        "      if eleccion == '1':\n",
        "        print('¬°Genial! Cu√©ntame qu√© te gustar√≠a ver: ')\n",
        "        to_search = input()\n",
        "        activitySearch(to_search,model2,db_films,2)\n",
        "      elif eleccion == '2':\n",
        "        print('¬°Genial! Cu√©ntame qu√© te gustar√≠a jugar: ')\n",
        "        to_search = input()\n",
        "        activitySearch(to_search,model2,db_boardgames,2)\n",
        "      elif eleccion == '3':\n",
        "        print('¬°Genial! Cu√©ntame qu√© te gustar√≠a leer: ')\n",
        "        to_search = input()\n",
        "        activitySearch(to_search,model2,db_books,2)\n",
        "      else:\n",
        "        output.clear()\n",
        "        print('¬°De acuerdo! M√°s tarde quiz√°s üòä.')\n",
        "    else:\n",
        "      output.clear()\n",
        "      print('¬°De acuerdo! M√°s tarde quiz√°s üòä.')\n",
        "  elif emocion_determinada == -1:\n",
        "    if red_flag:\n",
        "      print('Lamento escuchar eso ü•∫üåß')\n",
        "      print('Recuerda que siempre es importante hablar con alguien antes de tomar decisiones dr√°sticas.')\n",
        "      print('Vamos a intentar alegrar el d√≠a, ¬øqu√© te parece ver una pel√≠cula?‚ú®ü§π‚Äç‚ôÄÔ∏èüé•')\n",
        "    else:\n",
        "      print('Lamento escuchar eso ü•∫üåß. Vamos a intentar alegrar el d√≠a, ¬øqu√© te parece ver una pel√≠cula?‚ú®ü§π‚Äç‚ôÄÔ∏èüé•')\n",
        "    rta = yesNoValidator()\n",
        "    if rta:\n",
        "      output.clear()\n",
        "      print('¬°Genial! Cu√©ntame qu√© te gustar√≠a ver: ')\n",
        "      to_search = input()\n",
        "      activitySearch(to_search,model2,db_films,1)\n",
        "    else:\n",
        "      output.clear()\n",
        "      if red_flag:\n",
        "        print('¬°De acuerdo! M√°s tarde quiz√°s üòä.')\n",
        "        print('Recuerda que siempre es importante hablar con alguien antes de tomar decisiones dr√°sticas.')\n",
        "        print('No estas solo. Wilson esta con vos.')\n",
        "      else:\n",
        "        print('¬°De acuerdo! M√°s tarde quiz√°s üòä.')\n",
        "  else:\n",
        "    print('¬°Me alegra oir eso! Entonces, dime algo que te inspire en este momento, y te dar√© varias actividades para realizar en base a eso ;). ¬øQu√© te parece?')\n",
        "    rta = yesNoValidator()\n",
        "    if rta:\n",
        "      output.clear()\n",
        "      print('¬°Genial! Cu√©ntame sobre que quieres ver, leer o jugar: ')\n",
        "      to_search = input()\n",
        "      print('####################################################')\n",
        "      print('Top 3 peliculas que coinciden con lo que queres ver')\n",
        "      activitySearch(to_search,model2,db_films,3)\n",
        "      print('####################################################')\n",
        "      print('Top 3 juegos de mesa que coinciden con lo que queres ver')\n",
        "      activitySearch(to_search,model2,db_boardgames,3)\n",
        "      print('####################################################')\n",
        "      print('Top 3 libros que coinciden con lo que queres ver')\n",
        "      activitySearch(to_search,model2,db_books,3)\n",
        "      print('####################################################')\n",
        "\n",
        "    else:\n",
        "      output.clear()\n",
        "      print('¬°De acuerdo! M√°s tarde quiz√°s üòä.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6VIuFg8n38N"
      },
      "outputs": [],
      "source": [
        "def menu(intro : bool =True):\n",
        "  \"\"\"\n",
        "  Inicializa el men√∫ principal del sistema WILSON\n",
        "  \"\"\"\n",
        "  if intro:\n",
        "\n",
        "    loading_animation()\n",
        "    output.clear()\n",
        "\n",
        "    print('==============================================\\n|||||||||||||||||TP 1 - NLP|||||||||||||||||||\\n==============================================\\n')\n",
        "    time.sleep(0.3)\n",
        "    print('Integrantes: ')\n",
        "    time.sleep(0.3)\n",
        "    print('Alsop Agust√≠n')\n",
        "    time.sleep(0.3)\n",
        "    print('Hachen Roc√≠o')\n",
        "    time.sleep(2)\n",
        "    output.clear()\n",
        "    print(\"=\" * 42)\n",
        "    print( \"üåü‚ú®  Bienvenido al sistema WILSON  ‚ú®üåü\")\n",
        "    print(\"=\" * 42)\n",
        "    time.sleep(2)\n",
        "    output.clear()\n",
        "\n",
        "  info_horaria = saludo()\n",
        "\n",
        "  print(f\"Hora actual: {info_horaria[0]}\")\n",
        "  print(f'{info_horaria[1]} Soy WILSON ü§ñ te ayudar√© a decidir qu√© hacer hoy.')\n",
        "  user_input = input('¬øC√≥mo te sientes hoy?\\n')\n",
        "  output.clear()\n",
        "  respuestaEmocion(analisisEmocion(user_input,modelo_LR,model),redFlagIdentifier(user_input))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5QuK1xVoarq"
      },
      "outputs": [],
      "source": [
        "def start(intro=True) -> None:\n",
        "  \"\"\"\n",
        "  Esta funci√≥n inicia el sistema WILSON llamando a la funci√≥n menu()\n",
        "  \"\"\"\n",
        "  menu(intro)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Om_nQc7f_xu"
      },
      "source": [
        "## Bases de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5yFLbegggFz"
      },
      "source": [
        "### Base de datos para las emociones\n",
        "No necesita ninguna transformaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1NhTOYfL1rDCvJkFGbPFGNCALV_NfdlhDSud2-V51v7s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EstpGBY5WYnY",
        "outputId": "da1a7178-0cfc-4491-a4a2-fecae6d1bcbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1NhTOYfL1rDCvJkFGbPFGNCALV_NfdlhDSud2-V51v7s\n",
            "From (redirected): https://docs.google.com/spreadsheets/d/1NhTOYfL1rDCvJkFGbPFGNCALV_NfdlhDSud2-V51v7s/export?format=xlsx\n",
            "To: /content/NLP data base.xlsx\n",
            "\r0.00B [00:00, ?B/s]\r25.7kB [00:00, 37.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3EKH1gxgn_1"
      },
      "outputs": [],
      "source": [
        "database = '/content/NLP data base.xlsx'\n",
        "db_emotion = pd.read_excel(database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lx0gtafgn5N"
      },
      "outputs": [],
      "source": [
        "db_emotion['animo'] = db_emotion['animo'].map({\n",
        "    'triste': -1,\n",
        "    'neutro': 0,\n",
        "    'feliz': 1\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparaci√≥n de las bases de datos\n",
        "Pel√≠culas, Libros, Juegos de mesas\n",
        "\n",
        "Esto se realiza una sola vez para generar archivos json donde contendran toda la informaci√≥n necesaria para luego simplemente descargando ese json ejecutar la aplicaci√≥n"
      ],
      "metadata": {
        "id": "TmTM9p1pUiho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Descarga"
      ],
      "metadata": {
        "id": "5eyejChYUq6V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YHljXXoGn1Ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1204dc23-7a7b-49d9-bdaa-8e5a84c72b2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Descargamos los 2 datasets\\n!gdown 1YCu3xhZq4C5dYyekiluMabwyWBqQyd2c\\n!gdown 1yIWOgUV5WyskQvmq48QvF2Lzr0LxpAdq\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Descargamos los 2 datasets\n",
        "!gdown 1YCu3xhZq4C5dYyekiluMabwyWBqQyd2c\n",
        "!gdown 1yIWOgUV5WyskQvmq48QvF2Lzr0LxpAdq\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxcRhXuX4yl9"
      },
      "source": [
        "#### Pel√≠culas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5skj2AQ2hEqf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1c81bc65-208f-47b1-cf98-453a54cb7d54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\npeliculas = '/content/IMDB-Movie-Data.csv'\\ndb_films = pd.read_csv(peliculas)\\ndb_films\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "\"\"\"\n",
        "peliculas = '/content/IMDB-Movie-Data.csv'\n",
        "db_films = pd.read_csv(peliculas)\n",
        "db_films\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Realizamos el embedding de BERT para cada descripci√≥n\n",
        "db_films['tensor'] = db_films['Description'].apply(lambda x: model2.encode(x, convert_to_tensor=True))\n",
        "\n",
        "# Convertir la columna de tensores a listas\n",
        "db_films['tensor'] = db_films['tensor'].apply(lambda x: [float(val) for val in x])\n",
        "\n",
        "# Exportar a JSON\n",
        "db_films.to_json('folder/subfolder/db_films.json', orient='records', lines=True)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "O3sjxnthspYo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a8ec57fd-ca4d-4354-c17a-3982d28f3716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Realizamos el embedding de BERT para cada descripci√≥n\\ndb_films['tensor'] = db_films['Description'].apply(lambda x: model2.encode(x, convert_to_tensor=True))\\n\\n# Convertir la columna de tensores a listas\\ndb_films['tensor'] = db_films['tensor'].apply(lambda x: [float(val) for val in x])\\n\\n# Exportar a JSON\\ndb_films.to_json('folder/subfolder/db_films.json', orient='records', lines=True)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58S64Yk343aa"
      },
      "source": [
        "#### Juegos de mesa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jjRdtD-PoB2z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bd95301d-7245-46db-b872-7dda22be7980"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\njuego_mesa = \\'/content/bgg_database.csv\\'\\ndb_boardgames = pd.read_csv(juego_mesa)\\ndb_boardgames = db_boardgames.rename(columns={\"game_name\": \"Title\", \"description\": \"Description\"})\\ndb_boardgames\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "\"\"\"\n",
        "juego_mesa = '/content/bgg_database.csv'\n",
        "db_boardgames = pd.read_csv(juego_mesa)\n",
        "db_boardgames = db_boardgames.rename(columns={\"game_name\": \"Title\", \"description\": \"Description\"})\n",
        "db_boardgames\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Realizamos el embedding de BERT para cada descripci√≥n\n",
        "db_boardgames['tensor'] = db_boardgames['Description'].apply(lambda x: model2.encode(x, convert_to_tensor=True))\n",
        "\n",
        "# Convertir la columna de tensores a listas\n",
        "db_boardgames['tensor'] = db_boardgames['tensor'].apply(lambda x: [float(val) for val in x])\n",
        "\n",
        "# Exportar a JSON\n",
        "db_boardgames.to_json('folder/subfolder/db_boardgames.json', orient='records', lines=True)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BxzpxDsXsDwG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "5bd0d932-5c91-442b-d304-503a03a7f42a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Realizamos el embedding de BERT para cada descripci√≥n\\ndb_boardgames['tensor'] = db_boardgames['Description'].apply(lambda x: model2.encode(x, convert_to_tensor=True))\\n\\n# Convertir la columna de tensores a listas\\ndb_boardgames['tensor'] = db_boardgames['tensor'].apply(lambda x: [float(val) for val in x])\\n\\n# Exportar a JSON\\ndb_boardgames.to_json('folder/subfolder/db_boardgames.json', orient='records', lines=True)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55Sw6E5C46cL"
      },
      "source": [
        "#### Libros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16IJdVg-TzzL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "8e97380a-d830-41f1-ebaa-a4653f9fb832"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#URL de la p√°gina de Gutenberg con el top 1000 libros\\nurl = \\'https://www.gutenberg.org/browse/scores/top1000.php#books-last1\\'\\nresponse = requests.get(url)\\n\\n# Verificamos si la respuesta fue exitosa\\nif response.status_code == 200:\\n\\n    # Obtenemos los el contenido de la p√°gina, especificamente donde estan todos los titulos\\n    soup = BeautifulSoup(response.text, \\'html.parser\\')\\n    books_list = soup.select(\\'body > div.container > div > ol:nth-child(6) > li > a\\')\\n\\n    # Lista para almacenar los datos de cada libro\\n    books_data = []\\n\\n    # Iteramos sobre cada libro en la lista\\n    for book in books_list:\\n        # Extraemos el fragmento donde tiene la url para cargar la p√°gina del libro en espec√≠fico\\n        book_url = f\"https://www.gutenberg.org{book[\\'href\\']}\"\\n\\n        # Obtener el contenido de la p√°gina del libro\\n        response = requests.get(book_url)\\n        soup = BeautifulSoup(response.content, \"html.parser\")\\n\\n        # Buscamos la tabla que contiene el t√≠tulo y la descripci√≥n\\n        table = soup.find(\"table\", {\"class\": \"bibrec\"})\\n\\n        # Extraemos el libro, buscando que el header sea Title\\n        title_row = table.find(\"th\", text=\"Title\")\\n        if title_row:\\n            title = title_row.find_next_sibling(\"td\").text.strip()\\n        else:\\n            title = \"No se encontr√≥ el t√≠tulo.\"\\n\\n        # Extraemos el resumen, buscando que el header sea Summary\\n        summary_row = table.find(\"th\", text=\"Summary\")\\n        if summary_row:\\n            summary = summary_row.find_next_sibling(\"td\").text.strip()\\n        else:\\n            summary = \"No se encontr√≥ el resumen.\"\\n\\n        # Cargamos el t√≠tulo y la descripci√≥n a una lista de diccionarios\\n        books_data.append({\\n            \\'Title\\': title,\\n            \\'Description\\': summary\\n        })\\n\\n    # Creamos un DataFrame con los datos recopilados\\n    db_books = pd.DataFrame(books_data)\\n\\nelse:\\n    print(f\"Error al acceder a la p√°gina principal. C√≥digo de estado: {response.status_code}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "\"\"\"\n",
        "#URL de la p√°gina de Gutenberg con el top 1000 libros\n",
        "url = 'https://www.gutenberg.org/browse/scores/top1000.php#books-last1'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verificamos si la respuesta fue exitosa\n",
        "if response.status_code == 200:\n",
        "\n",
        "    # Obtenemos los el contenido de la p√°gina, especificamente donde estan todos los titulos\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    books_list = soup.select('body > div.container > div > ol:nth-child(6) > li > a')\n",
        "\n",
        "    # Lista para almacenar los datos de cada libro\n",
        "    books_data = []\n",
        "\n",
        "    # Iteramos sobre cada libro en la lista\n",
        "    for book in books_list:\n",
        "        # Extraemos el fragmento donde tiene la url para cargar la p√°gina del libro en espec√≠fico\n",
        "        book_url = f\"https://www.gutenberg.org{book['href']}\"\n",
        "\n",
        "        # Obtener el contenido de la p√°gina del libro\n",
        "        response = requests.get(book_url)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Buscamos la tabla que contiene el t√≠tulo y la descripci√≥n\n",
        "        table = soup.find(\"table\", {\"class\": \"bibrec\"})\n",
        "\n",
        "        # Extraemos el libro, buscando que el header sea Title\n",
        "        title_row = table.find(\"th\", text=\"Title\")\n",
        "        if title_row:\n",
        "            title = title_row.find_next_sibling(\"td\").text.strip()\n",
        "        else:\n",
        "            title = \"No se encontr√≥ el t√≠tulo.\"\n",
        "\n",
        "        # Extraemos el resumen, buscando que el header sea Summary\n",
        "        summary_row = table.find(\"th\", text=\"Summary\")\n",
        "        if summary_row:\n",
        "            summary = summary_row.find_next_sibling(\"td\").text.strip()\n",
        "        else:\n",
        "            summary = \"No se encontr√≥ el resumen.\"\n",
        "\n",
        "        # Cargamos el t√≠tulo y la descripci√≥n a una lista de diccionarios\n",
        "        books_data.append({\n",
        "            'Title': title,\n",
        "            'Description': summary\n",
        "        })\n",
        "\n",
        "    # Creamos un DataFrame con los datos recopilados\n",
        "    db_books = pd.DataFrame(books_data)\n",
        "\n",
        "else:\n",
        "    print(f\"Error al acceder a la p√°gina principal. C√≥digo de estado: {response.status_code}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Realizamos el embedding de BERT para cada descripci√≥n\n",
        "db_books['tensor'] = db_books['Description'].apply(lambda x: model2.encode(x, convert_to_tensor=True))\n",
        "\n",
        "# Convertir la columna de tensores a listas\n",
        "#db_books['tensor'] = db_books['tensor'].apply(lambda x: [float(val) for val in x])\n",
        "\n",
        "# Exportar a JSON\n",
        "#db_books.to_json('folder/subfolder/db_books.json', orient='records', lines=True)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jNUn3u_-qxNC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f8c0be1e-568e-4cca-c4b9-63778f0d8244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Realizamos el embedding de BERT para cada descripci√≥n\\ndb_books['tensor'] = db_books['Description'].apply(lambda x: model2.encode(x, convert_to_tensor=True))\\n\\n# Convertir la columna de tensores a listas\\n#db_books['tensor'] = db_books['tensor'].apply(lambda x: [float(val) for val in x])\\n\\n# Exportar a JSON\\n#db_books.to_json('folder/subfolder/db_books.json', orient='records', lines=True)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carga de base de datos ya tranformadas\n",
        "Pel√≠culas, Libros, Juegos de mesas"
      ],
      "metadata": {
        "id": "7N8nQgTlU2pM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Descarga"
      ],
      "metadata": {
        "id": "Q5m50oH7Vg1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1lzZgGVRZ6LhiZCNT06OhqIUjcrhFCqmo\n",
        "!gdown 1A9bzLQ1SOBja8_6TiOnGADHtTAjcOaXC\n",
        "!gdown 1TaqmkromGRIauqH6KV4MdNhGtsw7QaoJ"
      ],
      "metadata": {
        "id": "o5EmziOHV3ey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34268708-31fe-4374-82e8-56b155c35350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lzZgGVRZ6LhiZCNT06OhqIUjcrhFCqmo\n",
            "To: /content/db_films.json\n",
            "100% 7.31M/7.31M [00:00<00:00, 37.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1A9bzLQ1SOBja8_6TiOnGADHtTAjcOaXC\n",
            "To: /content/db_boardgames.json\n",
            "100% 8.96M/8.96M [00:00<00:00, 59.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TaqmkromGRIauqH6KV4MdNhGtsw7QaoJ\n",
            "To: /content/db_books.json\n",
            "100% 8.12M/8.12M [00:00<00:00, 36.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pel√≠culas"
      ],
      "metadata": {
        "id": "k1iac-Z4Viw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_films = pd.read_json('/content/db_films.json', orient='records', lines=True)\n",
        "db_films['tensor'] = db_films['tensor'].apply(lambda x: torch.tensor(x))"
      ],
      "metadata": {
        "id": "gVEq5HI3U7kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Juego de Mesa"
      ],
      "metadata": {
        "id": "NIXol16fVnmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_boardgames = pd.read_json('/content/db_boardgames.json', orient='records', lines=True)\n",
        "db_boardgames['tensor'] = db_boardgames['tensor'].apply(lambda x: torch.tensor(x))"
      ],
      "metadata": {
        "id": "lrMgjQLjU7ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Libros"
      ],
      "metadata": {
        "id": "A5_KhuyAVw-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_books = pd.read_json('/content/db_books.json', orient='records', lines=True)\n",
        "db_books['tensor'] = db_books['tensor'].apply(lambda x: torch.tensor(x))"
      ],
      "metadata": {
        "id": "1WQPkjjlU7Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9mJ9105hKqI"
      },
      "source": [
        "## Creaci√≥n del modelo para el estado de √°nimo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rc0-Cvg6pR8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0691d8cd-8f68-47b7-a1db-dddd0a2f6ebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ES1HzfQrKSVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a22439-a8fc-4872-e13e-4146617a5c86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisi√≥n Regresi√≥n Log√≠stica: 0.7472527472527473\n",
            "Reporte de clasificaci√≥n Regresi√≥n Log√≠stica:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.84      0.73      0.78        37\n",
            "           0       0.69      0.71      0.70        28\n",
            "           1       0.70      0.81      0.75        26\n",
            "\n",
            "    accuracy                           0.75        91\n",
            "   macro avg       0.74      0.75      0.74        91\n",
            "weighted avg       0.76      0.75      0.75        91\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "X = db_emotion[\"frase\"].str.lower()\n",
        "y = db_emotion[\"animo\"].to_numpy() # Convertimos a numpy array\n",
        "\n",
        "# Divisi√≥n del dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Obtenemos los embeddings de BERT para los conjuntos de entrenamiento y prueba\n",
        "X_train_vectorized = model.encode(X_train.tolist())\n",
        "X_test_vectorized = model.encode(X_test.tolist())\n",
        "\n",
        "# Creaci√≥n y entrenamiento del modelo de Regresi√≥n Log√≠stica Multinomial\n",
        "modelo_LR = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
        "modelo_LR.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Evaluaci√≥n del modelo de Regresi√≥n Log√≠stica\n",
        "y_pred_LR = modelo_LR.predict(X_test_vectorized)\n",
        "acc_LR = accuracy_score(y_test, y_pred_LR)\n",
        "report_LR = classification_report(y_test, y_pred_LR, zero_division=1)\n",
        "\n",
        "print(\"Precisi√≥n Regresi√≥n Log√≠stica:\", acc_LR)\n",
        "print(\"Reporte de clasificaci√≥n Regresi√≥n Log√≠stica:\\n\", report_LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPrnuz_IkVi0"
      },
      "source": [
        "## Metodolog√≠a de selecci√≥n de un juego, pelicula o libro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TEtDwKmAK5Q"
      },
      "source": [
        "### SBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mEw0afdA9w-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69eb01da-3ac5-4a7d-f792-9b0253b437ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model2 = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFHgyXA7oPk6"
      },
      "source": [
        "## C√≥digo Principal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ys8pm8b-P59_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3c1ab6f-5e57-4f50-e307-e797bb313f9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¬°Genial! Cu√©ntame sobre que quieres ver, leer o jugar: \n",
            "una cancion \n",
            "####################################################\n",
            "Top 3 peliculas que coinciden con lo que queres ver\n",
            "B√∫squeda: una cancion \n",
            "\n",
            "Encontr√© estos resultados: \n",
            "T√≠tulo: Happy Feet\n",
            "Descripci√≥n: Into the world of the Emperor Penguins, who find their soul mates through song, a penguin is born who cannot sing. But he can tap dance something fierce!\n",
            "Similitud: 0.2058\n",
            "\n",
            "T√≠tulo: Mamma Mia!\n",
            "Descripci√≥n: The story of a bride-to-be trying to find her real father told using hit songs by the popular '70s group ABBA.\n",
            "Similitud: 0.1755\n",
            "\n",
            "T√≠tulo: Begin Again\n",
            "Descripci√≥n: A chance encounter between a disgraced music-business executive and a young singer-songwriter new to Manhattan turns into a promising collaboration between the two talents.\n",
            "Similitud: 0.1688\n",
            "\n",
            "####################################################\n",
            "Top 3 juegos de mesa que coinciden con lo que queres ver\n",
            "B√∫squeda: una cancion \n",
            "\n",
            "Encontr√© estos resultados: \n",
            "T√≠tulo: HITSTER\n",
            "Descripci√≥n: Create an instant party with HITSTER, the music card game of the century!\n",
            "Listen to over 100 years of amazing hits, take turns arranging them in chronological order on your music timeline and create your trip down memory lane. \n",
            "The first player to collect 10 hits is crowned the HITSTER\n",
            "\n",
            "HITSTER is the perfect game for an evening with lots of laughing, singing, dancing and sharing memories. Simply open the box, scan a song card and let the music do the rest to create an instant party. So what are you waiting for?\n",
            "\n",
            "How to play\n",
            "1. Pick a music card and scan the QR code with the free Hitster app to automatically play it in Spotify\n",
            "2. Guess when the song was released by placing it in the right position of your music timeline.  \n",
            "3. Flip the music card. If correct, you keep the music card to build your timeline.\n",
            "Tip: add the Hitster tokens into the mix for an even more exciting game\n",
            "\n",
            "Features:\n",
            "- With more than 300 of the greatest hits HITSTER will create an instant party for everybody&hellip;guaranteed!\n",
            "- Everybody is invited for the HITSTER party! Super easy rules and no game set-up gets the party started in no-time. Simply scan a card with the free Hitster app and the music starts playing automatically in Spotify, isn't that cool?\n",
            "- HITSTER is the music game in which you don&rsquo;t have to be a music expert. Simply guess if a song was released before or after other songs in your music timeline. The futher you are in building your timeline the more challenging it becomes. Your chances of winning increases if you can also name the artists and song titles. \n",
            "- Are you craving to show of your music knowledge? Play HITSTER with the Pro rules or even Expert rules in which you have to know the exact year, artist and song title.\n",
            "- Fancy a less competitive trip down memory lane? Then HITSTER can also be played as one team or by yourself\n",
            "\n",
            "&mdash;description from the publisher\n",
            "\n",
            "\n",
            "Similitud: 0.1323\n",
            "\n",
            "T√≠tulo: Time's Up! Title Recall!\n",
            "Descripci√≥n: Based on the popular game Time's Up!, Time's Up: Title Recall challenges players to guess the titles of books, films, songs, and more. Players try to get their teammates to guess the same set of titles over three rounds. In each round, one member of a team tries to get their teammates to guess as many titles as possible in 30 seconds. In round 1, almost any kind of clue is allowed. In round 2, no more than one word can be used in each clue (but unlimited sounds and gestures are permitted.) In round 3, no words are allowed at all.\n",
            "\n",
            "\n",
            "Similitud: 0.0725\n",
            "\n",
            "T√≠tulo: Letter Jam\n",
            "Descripci√≥n: Letter Jam is a 2-6 player cooperative word game where players assist each other in composing meaningful words from letters around the table. The trick is holding the letter card so that it&rsquo;s only visible to other players and not to you.\n",
            "\n",
            "At the start of the game, each player receives a set of face-down letter cards that can be arranged to form an existing word. The setup can be prepared by using a special card scanning app, or by players selecting words for each other. Each player then puts their first card in their stand facing the other players without looking at it, and the game begins.\n",
            "\n",
            "The game is played in turns. Each turn, players simultaneously search other players&rsquo; letters to see what words they can spell out (telling the others the length of the word they can make up). The player who offers the longest word can then be chosen as the clue giver.\n",
            "\n",
            "The clue giver spells out their clue by putting numbered tokens in front of the other players. Number one goes to the player whose letter comes first in the clue, number two to the second letter etc. They can always use a wild card which can be any letter, but they cannot tell others which letter it represents.\n",
            "\n",
            "Each player with a numbered token (or tokens) in front of them then tries to figure out what their letter is. If they do, they place the card face down before revealing the next letter.  At the end of the game, players can then rearrange the cards to try to form an existing word. All players then reveal their cards to see if they were successful or not. The more players who have an existing word in front of them, the bigger their collective success.\n",
            "\n",
            "&mdash;description from the publisher\n",
            "\n",
            "\n",
            "Similitud: 0.0676\n",
            "\n",
            "####################################################\n",
            "Top 3 libros que coinciden con lo que queres ver\n",
            "B√∫squeda: una cancion \n",
            "\n",
            "Encontr√© estos resultados: \n",
            "T√≠tulo: Cowboy Songs, and Other Frontier Ballads\n",
            "Descripci√≥n: \"Cowboy Songs and Other Frontier Ballads\" by Various is a collection of folk songs and ballads that captures the essence of American cowboy culture during the late 19th to early 20th century. This anthology reflects the life, struggles, and emotions of cowboys, detailing their adventures, heartaches, and the rugged landscape of the West. The songs illustrate the camaraderie among cowboys, their love for freedom, and the challenges they faced in their profession.  The opening portion of the collection features an introduction that highlights the importance of preserving these ballads as a vital part of American folklore. It discusses the influence of the Anglo-Saxon ballad tradition in the Southwest and how these songs were created and passed down through oral recital among cowboys and other frontier folk. Notable themes include love, loss, the cowboy‚Äôs relationship with nature, and the rough lifestyle associated with cattle herding. Through the vivid imagery and emotional depth of the lyrics, readers gain insight into the unique spirit of the cowboy, who captivates through both his bravery and vulnerability. (This is an automatically generated summary.)\n",
            "Similitud: 0.1956\n",
            "\n",
            "T√≠tulo: The Song Celestial; Or, Bhagavad-G√Æt√¢ (from the Mah√¢bh√¢rata)Being a discourse between Arjuna, Prince of India, and the Supreme Being under the form of Krishna\n",
            "Descripci√≥n: \"The Song Celestial; Or, Bhagavad-G√Æt√¢ (from the Mah√¢bh√¢rata)\" by Sir Edwin Arnold is a philosophical poem and spiritual discourse, likely written in the late 19th century. The text presents a dialogue between Prince Arjuna, a warrior facing a profound moral dilemma on the battlefield of Kurukshetra, and Krishna, who embodies the Supreme Being and serves as his charioteer. This discourse explores themes of duty, righteousness, and the nature of life and death, seeking to impart wisdom on both personal and cosmic levels.  The opening of the work introduces the pivotal moment in which Arjuna surveys the battle and becomes overwhelmed with grief and compassion for his relatives on both sides of the conflict. He articulates his fears and moral concerns about fighting against kinsmen, questioning the purpose and morality of war itself. As he grapples with this turmoil, Krishna responds with profound guidance, urging Arjuna to overcome his doubts and embrace his duty as a warrior, emphasizing the eternal nature of the soul and the importance of righteous action. This sets the stage for the philosophical journey that will unfold throughout the text. (This is an automatically generated summary.)\n",
            "Similitud: 0.1712\n",
            "\n",
            "T√≠tulo: Lyrical Ballads, With a Few Other Poems (1798)\n",
            "Descripci√≥n: \"Lyrical Ballads, With a Few Other Poems\" by Wordsworth and Coleridge is a collection of poetry from the late 18th century that marks a significant turning point in English literature, being a cornerstone of the Romantic movement. This work primarily explores the lives, emotions, and experiences of common people, employing language accessible to the middle and lower classes, which diverges from the ornate style of earlier poetry. The poets aim to provoke a deeper appreciation of nature and the human condition through their verses.  The opening of the collection introduces the intention behind the poems and the methodologies employed by the authors. They express a desire to experiment with conversational language while depicting human emotions in a naturalistic manner. The beginning discusses various poems within the collection, mentioning characters such as the titular Ancient Mariner, who shares harrowing tales of his maritime experiences, and Goody Blake, a poor woman whose actions lead to mysterious consequences. This initial section sets the stage for readers to encounter various narratives that reflect the intertwined relationships between humanity and nature, as well as the complexities of life itself. (This is an automatically generated summary.)\n",
            "Similitud: 0.1511\n",
            "\n",
            "####################################################\n"
          ]
        }
      ],
      "source": [
        "start()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}