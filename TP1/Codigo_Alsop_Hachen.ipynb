{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV86mNULfArI"
      },
      "source": [
        "# Procesamiento del Lenguaje Natural - TUIA\n",
        "---\n",
        "## Trabajo Práctico 1 - Clasificador de Recomendaciones Recreativas utilizando NLP\n",
        "\n",
        "Integrantes:\n",
        "- Alsop Agustín (A-4651/7)\n",
        "- Hachen Rocío (H-1184/3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMj7eJYhM0HQ"
      },
      "source": [
        "### Instalaciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cfWhlARYM2PP",
        "outputId": "57fe2e91-301e-463c-df5f-680d6762b668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-lg==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.7.0/es_core_news_lg-3.7.0-py3-none-any.whl (568.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m821.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-lg==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download es_core_news_lg\n",
        "!pip install sentence_transformers\n",
        "#!pip install thefuzz python-Levenshtein\n",
        "#!pip install transformers sentence_transformers\n",
        "#!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO5umZA5B-cz"
      },
      "source": [
        "Se recomienda reiniciar el entorno luego de las instalaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLmDWAGXfsm0"
      },
      "source": [
        "## Librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n46AqKzkoHK-"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "#import nltk\n",
        "#from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "#from thefuzz import process\n",
        "#import requests\n",
        "#from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "from google.colab import output\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "from datetime import datetime\n",
        "import pytz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtWTGOcQMTgE"
      },
      "source": [
        "## Funciones\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU8U31aYNIQO"
      },
      "source": [
        "### Identificación de frases problemáticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tduUuMaLmzf"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"es_core_news_lg\")\n",
        "\n",
        "frases_peligrosas = [\n",
        "    \"suicidio\", \"suicidarse\", \"no quiero vivir\", \"quiero morir\", \"me voy a matar\",\n",
        "    \"pegarse un tiro\", \"matarme\", \"acabar con todo\", \"lastimarme\", \"me hago daño\",\n",
        "    \"me quiero cortar\", \"terminarlo todo\", \"me odio\", \"quiero desaparecer\",\n",
        "    \"no puedo seguir así\", \"me quiero morir\", \"ojalá desapareciera\",\n",
        "    \"me voy de este mundo\", \"no le importo a nadie\", \"no tengo a nadie\",\n",
        "    \"nadie se preocupa por mí\", \"ya no aguanto más\", \"hasta aquí llegué\",\n",
        "    \"mi vida no vale nada\", \"ojalá no despertara\", \"no tengo miedo a morir\",\n",
        "    \"un descanso eterno\", \"tirar por un puente\", \"me quiero tirar por un puente\"\n",
        "]\n",
        "\n",
        "def redFlagIdentifier(frase : str) -> bool:\n",
        "    \"\"\"\n",
        "    Verifica si una frase ingresada contiene indicios de pensamientos o expresiones relacionadas con autolesiones\n",
        "    \"\"\"\n",
        "\n",
        "    doc_frase = nlp(frase.lower())\n",
        "\n",
        "    # Verificamos la similitud con las frases peligrosas\n",
        "    for frase_peligrosa in frases_peligrosas:\n",
        "        doc_peligroso = nlp(frase_peligrosa)\n",
        "        if doc_frase.similarity(doc_peligroso) > 0.7:  # Umbral de similitud\n",
        "            return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQvTOeC45Lo5"
      },
      "source": [
        "#### Test de la función"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmmlujKZOZID",
        "outputId": "32035ab6-d13a-44c2-c719-fd5d961c0451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frase: 'me voy a suicidar' - ¿Peligrosa?: True\n",
            "Frase: 'no quiero vivir más' - ¿Peligrosa?: True\n",
            "Frase: 'hoy es un mal día' - ¿Peligrosa?: False\n",
            "Frase: 'no aguanto esto' - ¿Peligrosa?: True\n",
            "Frase: 'me quiero tirar por un puente' - ¿Peligrosa?: True\n",
            "Frase: 'la vida no tiene sentido' - ¿Peligrosa?: False\n",
            "Frase: 'todos estarían mejor sin mí' - ¿Peligrosa?: True\n",
            "Frase: 'me siento vacío' - ¿Peligrosa?: True\n",
            "Frase: 'quiero acabar con todo' - ¿Peligrosa?: True\n",
            "Frase: 'ojalá desapareciera' - ¿Peligrosa?: True\n",
            "Frase: 'me voy de este mundo' - ¿Peligrosa?: True\n",
            "Frase: 'espero que esto termine pronto' - ¿Peligrosa?: True\n",
            "Frase: 'me siento atrapado' - ¿Peligrosa?: True\n",
            "Frase: 'la vida es demasiado difícil' - ¿Peligrosa?: False\n",
            "Frase: 'no tengo a nadie que me quiera' - ¿Peligrosa?: True\n",
            "Frase: 'voy a matar a alguien' - ¿Peligrosa?: True\n"
          ]
        }
      ],
      "source": [
        "frases_polemicas = [\n",
        "    \"me voy a suicidar\",\n",
        "    \"no quiero vivir más\",\n",
        "    \"hoy es un mal día\",\n",
        "    \"no aguanto esto\",\n",
        "    \"me quiero tirar por un puente\",\n",
        "    \"la vida no tiene sentido\",\n",
        "    \"todos estarían mejor sin mí\",\n",
        "    \"me siento vacío\",\n",
        "    \"quiero acabar con todo\",\n",
        "    \"ojalá desapareciera\",\n",
        "    \"me voy de este mundo\",\n",
        "\n",
        "    \"espero que esto termine pronto\",\n",
        "    \"me siento atrapado\",\n",
        "    \"la vida es demasiado difícil\",\n",
        "    \"no tengo a nadie que me quiera\",\n",
        "    \"voy a matar a alguien\"\n",
        "]\n",
        "\n",
        "# Iterar sobre las frases polémicas y verificar si se consideran peligrosas\n",
        "for frase in frases_polemicas:\n",
        "    resultado = redFlagIdentifier(frase)\n",
        "    print(f\"Frase: '{frase}' - ¿Peligrosa?: {resultado}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIFcg8MU5WkH"
      },
      "source": [
        "### Detección de emociones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waGH1eHqtnnY"
      },
      "outputs": [],
      "source": [
        "def analisisEmocion(input_usuario : str, modeloLR, modeloST) -> str :\n",
        "  \"\"\"\n",
        "  Analiza el estado emocional del usuario con base en el texto ingresado,\n",
        "  utilizando un modelo de regresión logística (modeloLR) para clasificación de emociones\n",
        "  y el modelo Sentence Transformer (modeloST) para transformar la frase en una representación vectorial.\n",
        "  \"\"\"\n",
        "  new_phrases = [input_usuario]\n",
        "\n",
        "  # Preprocesamiento y vectorización de las nuevas frases\n",
        "  new_phrases_lower = [text.lower() for text in new_phrases]\n",
        "  new_phrases_vectorized = modeloST.encode(new_phrases_lower)\n",
        "\n",
        "  # Haciendo predicciones con el modelo entrenado\n",
        "  new_predictions = modeloLR.predict(new_phrases_vectorized)\n",
        "\n",
        "  # Definimos el mapeo para las etiquetas\n",
        "  labels = {-1: 'triste', 0: 'neutro', 1: 'feliz'}\n",
        "\n",
        "  # Mostrando las predicciones junto con las frases\n",
        "  for text, label in zip(new_phrases, new_predictions):\n",
        "      print(f\"Texto: '{text}'\")\n",
        "      print(f\"Clasificación predicha: {labels[label]}\\n\")\n",
        "\n",
        "  return new_predictions[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewUpvxU150B9"
      },
      "source": [
        "### Funciones secundarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vuvQAaxctr0"
      },
      "outputs": [],
      "source": [
        "def yesNoValidator() -> bool:\n",
        "  \"\"\"\n",
        "  Valida respuestas de sí o no ingresadas por el usuario.\n",
        "  Acepta varias variantes de cada respuesta (como \"Y\", \"Yes\", \"Si\", etc.), verificando que la entrada sea válida antes de continuar.\n",
        "  \"\"\"\n",
        "\n",
        "  rta = input(\"Responda si o no (Y/N): \")\n",
        "  yes_answers = [\"Y\", \"y\", \"Yes\", \"Si\",\"S\",\"s\",\":)\"]\n",
        "  no_answers = [\"N\", \"n\", \"No\",\":(\"]\n",
        "  valid_answers = yes_answers + no_answers\n",
        "  while rta not in valid_answers:\n",
        "    print(f'Por favor conteste con las opciones válidas: {valid_answers}')\n",
        "    rta = input(\"Responda si o no (Y/N): \")\n",
        "  if rta in yes_answers:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHc9czbARool"
      },
      "outputs": [],
      "source": [
        "def saludo() -> tuple:\n",
        "  \"\"\"\n",
        "  Genera un saludo contextualizado según la hora local de Buenos Aires, Argentina,\n",
        "  devolviendo un mensaje de buenos días, buenas tardes o buenas noches, junto con la hora exacta en formato de 24 horas.\n",
        "  \"\"\"\n",
        "  local_timezone = pytz.timezone(\"America/Argentina/Buenos_Aires\")\n",
        "  hora_actual = datetime.now(local_timezone)\n",
        "\n",
        "  # Determina el saludo basado en la hora\n",
        "  if 6 <= hora_actual.hour < 12:\n",
        "      saludo = \"¡Buenos días!\"\n",
        "  elif 12 <= hora_actual.hour < 20:\n",
        "      saludo = \"¡Buenas tardes!\"\n",
        "  else:\n",
        "      saludo = \"¡Buenas noches!\"\n",
        "\n",
        "  hora_minutos = hora_actual.strftime(\"%H:%M\")\n",
        "  return hora_minutos, saludo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui_vH1ZlOPub"
      },
      "outputs": [],
      "source": [
        "def loading_animation() -> None:\n",
        "  \"\"\"\n",
        "  Esta función genera una animación de carga en la consola utilizando una secuencia de emojis de gatos y puntos, simulando una espera mientras el sistema procesa alguna tarea.\n",
        "  \"\"\"\n",
        "  loading_text = \"Espere un momento\"\n",
        "  animation_frames = [\"😺\", \"😸\", \"😹\", \"😻\", \"😼\", \"😽\", \"🙀\", \"😿\", \"😾\"]\n",
        "  for _ in range(2):\n",
        "      punto = \".\"\n",
        "      for frame in animation_frames:\n",
        "          punto += \".\"\n",
        "          clear_output(wait=True)\n",
        "          print(f\"{loading_text}{punto} \\n{frame}\")\n",
        "          time.sleep(0.3)\n",
        "  print(\"¡Carga completa!\")\n",
        "  time.sleep(0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8OXmZJUn_qq"
      },
      "outputs": [],
      "source": [
        "def activitySearch(prompt : str ,modelo ,db : pd.DataFrame ,quantity : int) -> None:\n",
        "  \"\"\"\n",
        "  Esta función busca actividades en una base de datos según la similitud semántica con el texto de entrada (prompt).\n",
        "  Utiliza generados por un modelo de transformers para identificar las actividades más relevantes.\n",
        "  \"\"\"\n",
        "  input_embeddings = model2.encode(prompt, convert_to_tensor=True)\n",
        "\n",
        "  db['coincidencia_sbert'] = db['tensor'].apply(lambda x: util.cos_sim(x, input_embeddings).item())\n",
        "\n",
        "  db = db.sort_values(by='coincidencia_sbert', ascending=False)\n",
        "  top_3_activity = db[['Title', 'Description', 'coincidencia_sbert']].head(quantity)\n",
        "\n",
        "  print(f'Búsqueda: {prompt}\\n')\n",
        "  print('Encontré estos resultados: ')\n",
        "  for index, row in top_3_activity.iterrows():\n",
        "    print(f\"Título: {row['Title']}\")\n",
        "    print(f\"Descripción: {row['Description']}\")\n",
        "    print(f\"Similitud: {row['coincidencia_sbert']:.4f}\\n\")\n",
        "  #db[['Title','Description', 'coincidencia_sbert']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ7V0gfl59hr"
      },
      "source": [
        "### Menu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9haGmEPacja"
      },
      "outputs": [],
      "source": [
        "def respuestaEmocion(emocion_determinada : int, red_flag : bool = False  ) -> None:\n",
        "  \"\"\"\n",
        "  Maneja la respuesta del sistema según la emoción determinada del usuario y si se detectó alguna frase de riesgo (red_flag).\n",
        "  Basándose en la emoción del usuario, ofrece recomendaciones de actividades que pueden mejorar su estado de ánimo.\n",
        "  \"\"\"\n",
        "  if emocion_determinada == 0:\n",
        "    print('Parece que el día se mantiene normal 😊. ¿Quieres una recomendación para el día de hoy?')\n",
        "    rta = yesNoValidator()\n",
        "    if rta:\n",
        "      output.clear()\n",
        "      print('¡Genial! El día se presta para cualquiera de estas actividades, ¿cuál prefieres?\\n Ingresá el número de la opción requerida')\n",
        "      print('1- Películas\\n2- Juegos de mesa\\n3- Libros\\n4- Salir')\n",
        "      eleccion = input()\n",
        "      while eleccion not in ['1','2','3','4']:\n",
        "        output.clear()\n",
        "        print('Por favor ingrese una opción válida')\n",
        "        print('¿Cuál prefieres?\\n Ingresá el número de la opción requerida 😊.')\n",
        "        print('1- Películas\\n2- Juegos de mesa\\n3- Libros\\n4- Salir')\n",
        "        eleccion = input()\n",
        "      if eleccion == '1':\n",
        "        print('¡Genial! Cuéntame qué te gustaría ver: ')\n",
        "        to_search = input()\n",
        "        activitySearch(to_search,model2,db_films,2)\n",
        "      elif eleccion == '2':\n",
        "        print('¡Genial! Cuéntame qué te gustaría jugar: ')\n",
        "        to_search = input()\n",
        "        activitySearch(to_search,model2,db_boardgames,2)\n",
        "      elif eleccion == '3':\n",
        "        print('¡Genial! Cuéntame qué te gustaría leer: ')\n",
        "        to_search = input()\n",
        "        activitySearch(to_search,model2,db_books,2)\n",
        "      else:\n",
        "        output.clear()\n",
        "        print('¡De acuerdo! Más tarde quizás 😊.')\n",
        "    else:\n",
        "      output.clear()\n",
        "      print('¡De acuerdo! Más tarde quizás 😊.')\n",
        "  elif emocion_determinada == -1:\n",
        "    if red_flag:\n",
        "      print('Lamento escuchar eso 🥺🌧')\n",
        "      print('Recuerda que siempre es importante hablar con alguien antes de tomar decisiones drásticas.')\n",
        "      print('Vamos a intentar alegrar el día, ¿qué te parece ver una película?✨🤹‍♀️🎥')\n",
        "    else:\n",
        "      print('Lamento escuchar eso 🥺🌧. Vamos a intentar alegrar el día, ¿qué te parece ver una película?✨🤹‍♀️🎥')\n",
        "    rta = yesNoValidator()\n",
        "    if rta:\n",
        "      output.clear()\n",
        "      print('¡Genial! Cuéntame qué te gustaría ver: ')\n",
        "      to_search = input()\n",
        "      activitySearch(to_search,model2,db_films,1)\n",
        "    else:\n",
        "      output.clear()\n",
        "      if red_flag:\n",
        "        print('¡De acuerdo! Más tarde quizás 😊.')\n",
        "        print('Recuerda que siempre es importante hablar con alguien antes de tomar decisiones drásticas.')\n",
        "        print('No estas solo. Wilson esta con vos.')\n",
        "      else:\n",
        "        print('¡De acuerdo! Más tarde quizás 😊.')\n",
        "  else:\n",
        "    print('¡Me alegra oir eso! Entonces, dime algo que te inspire en este momento, y te daré varias actividades para realizar en base a eso ;). ¿Qué te parece?')\n",
        "    rta = yesNoValidator()\n",
        "    if rta:\n",
        "      output.clear()\n",
        "      print('¡Genial! Cuéntame sobre que quieres ver, leer o jugar: ')\n",
        "      to_search = input()\n",
        "      print('####################################################')\n",
        "      print('Top 3 peliculas que coinciden con lo que queres ver')\n",
        "      activitySearch(to_search,model2,db_films,3)\n",
        "      print('####################################################')\n",
        "      print('Top 3 juegos de mesa que coinciden con lo que queres ver')\n",
        "      activitySearch(to_search,model2,db_boardgames,3)\n",
        "      print('####################################################')\n",
        "      print('Top 3 libros que coinciden con lo que queres ver')\n",
        "      activitySearch(to_search,model2,db_books,3)\n",
        "      print('####################################################')\n",
        "\n",
        "    else:\n",
        "      output.clear()\n",
        "      print('¡De acuerdo! Más tarde quizás 😊.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6VIuFg8n38N"
      },
      "outputs": [],
      "source": [
        "def menu(intro : bool =True):\n",
        "  \"\"\"\n",
        "  Inicializa el menú principal del sistema WILSON\n",
        "  \"\"\"\n",
        "  if intro:\n",
        "\n",
        "    loading_animation()\n",
        "    output.clear()\n",
        "\n",
        "    print('==============================================\\n|||||||||||||||||TP 1 - NLP|||||||||||||||||||\\n==============================================\\n')\n",
        "    time.sleep(0.3)\n",
        "    print('Integrantes: ')\n",
        "    time.sleep(0.3)\n",
        "    print('Alsop Agustín')\n",
        "    time.sleep(0.3)\n",
        "    print('Hachen Rocío')\n",
        "    time.sleep(2)\n",
        "    output.clear()\n",
        "    print(\"=\" * 42)\n",
        "    print( \"🌟✨  Bienvenido al sistema WILSON  ✨🌟\")\n",
        "    print(\"=\" * 42)\n",
        "    time.sleep(2)\n",
        "    output.clear()\n",
        "\n",
        "  info_horaria = saludo()\n",
        "\n",
        "  print(f\"Hora actual: {info_horaria[0]}\")\n",
        "  print(f'{info_horaria[1]} Soy WILSON 🤖 te ayudaré a decidir qué hacer hoy.')\n",
        "  user_input = input('¿Cómo te sientes hoy?\\n')\n",
        "  output.clear()\n",
        "  respuestaEmocion(analisisEmocion(user_input,modelo_LR,model),redFlagIdentifier(user_input))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5QuK1xVoarq"
      },
      "outputs": [],
      "source": [
        "def start(intro=True) -> None:\n",
        "  \"\"\"\n",
        "  Esta función inicia el sistema WILSON llamando a la función menu()\n",
        "  \"\"\"\n",
        "  menu(intro)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Om_nQc7f_xu"
      },
      "source": [
        "## Bases de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5yFLbegggFz"
      },
      "source": [
        "### Base de datos para las emociones\n",
        "No necesita ninguna transformación"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1NhTOYfL1rDCvJkFGbPFGNCALV_NfdlhDSud2-V51v7s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EstpGBY5WYnY",
        "outputId": "da1a7178-0cfc-4491-a4a2-fecae6d1bcbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1NhTOYfL1rDCvJkFGbPFGNCALV_NfdlhDSud2-V51v7s\n",
            "From (redirected): https://docs.google.com/spreadsheets/d/1NhTOYfL1rDCvJkFGbPFGNCALV_NfdlhDSud2-V51v7s/export?format=xlsx\n",
            "To: /content/NLP data base.xlsx\n",
            "\r0.00B [00:00, ?B/s]\r25.7kB [00:00, 37.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3EKH1gxgn_1"
      },
      "outputs": [],
      "source": [
        "database = '/content/NLP data base.xlsx'\n",
        "db_emotion = pd.read_excel(database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lx0gtafgn5N"
      },
      "outputs": [],
      "source": [
        "db_emotion['animo'] = db_emotion['animo'].map({\n",
        "    'triste': -1,\n",
        "    'neutro': 0,\n",
        "    'feliz': 1\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparación de las bases de datos\n",
        "Películas, Libros, Juegos de mesas\n",
        "\n",
        "Esto se realiza una sola vez para generar archivos json donde contendran toda la información necesaria para luego simplemente descargando ese json ejecutar la aplicación"
      ],
      "metadata": {
        "id": "TmTM9p1pUiho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Descarga"
      ],
      "metadata": {
        "id": "5eyejChYUq6V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YHljXXoGn1Ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1204dc23-7a7b-49d9-bdaa-8e5a84c72b2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Descargamos los 2 datasets\\n!gdown 1YCu3xhZq4C5dYyekiluMabwyWBqQyd2c\\n!gdown 1yIWOgUV5WyskQvmq48QvF2Lzr0LxpAdq\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Descargamos los 2 datasets\n",
        "!gdown 1YCu3xhZq4C5dYyekiluMabwyWBqQyd2c\n",
        "!gdown 1yIWOgUV5WyskQvmq48QvF2Lzr0LxpAdq\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxcRhXuX4yl9"
      },
      "source": [
        "#### Películas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5skj2AQ2hEqf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1c81bc65-208f-47b1-cf98-453a54cb7d54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\npeliculas = '/content/IMDB-Movie-Data.csv'\\ndb_films = pd.read_csv(peliculas)\\ndb_films\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "\"\"\"\n",
        "peliculas = '/content/IMDB-Movie-Data.csv'\n",
        "db_films = pd.read_csv(peliculas)\n",
        "db_films\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Realizamos el embedding de BERT para cada descripción\n",
        "db_films['tensor'] = db_films['Description'].apply(lambda x: model2.encode(x, convert_to_tensor=True))\n",
        "\n",
        "# Convertir la columna de tensores a listas\n",
        "db_films['tensor'] = db_films['tensor'].apply(lambda x: [float(val) for val in x])\n",
        "\n",
        "# Exportar a JSON\n",
        "db_films.to_json('folder/subfolder/db_films.json', orient='records', lines=True)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "O3sjxnthspYo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a8ec57fd-ca4d-4354-c17a-3982d28f3716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Realizamos el embedding de BERT para cada descripción\\ndb_films['tensor'] = db_films['Description'].apply(lambda x: model2.encode(x, convert_to_tensor=True))\\n\\n# Convertir la columna de tensores a listas\\ndb_films['tensor'] = db_films['tensor'].apply(lambda x: [float(val) for val in x])\\n\\n# Exportar a JSON\\ndb_films.to_json('folder/subfolder/db_films.json', orient='records', lines=True)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58S64Yk343aa"
      },
      "source": [
        "#### Juegos de mesa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jjRdtD-PoB2z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bd95301d-7245-46db-b872-7dda22be7980"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\njuego_mesa = \\'/content/bgg_database.csv\\'\\ndb_boardgames = pd.read_csv(juego_mesa)\\ndb_boardgames = db_boardgames.rename(columns={\"game_name\": \"Title\", \"description\": \"Description\"})\\ndb_boardgames\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "\"\"\"\n",
        "juego_mesa = '/content/bgg_database.csv'\n",
        "db_boardgames = pd.read_csv(juego_mesa)\n",
        "db_boardgames = db_boardgames.rename(columns={\"game_name\": \"Title\", \"description\": \"Description\"})\n",
        "db_boardgames\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Realizamos el embedding de BERT para cada descripción\n",
        "db_boardgames['tensor'] = db_boardgames['Description'].apply(lambda x: model2.encode(x, convert_to_tensor=True))\n",
        "\n",
        "# Convertir la columna de tensores a listas\n",
        "db_boardgames['tensor'] = db_boardgames['tensor'].apply(lambda x: [float(val) for val in x])\n",
        "\n",
        "# Exportar a JSON\n",
        "db_boardgames.to_json('folder/subfolder/db_boardgames.json', orient='records', lines=True)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BxzpxDsXsDwG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "5bd0d932-5c91-442b-d304-503a03a7f42a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Realizamos el embedding de BERT para cada descripción\\ndb_boardgames['tensor'] = db_boardgames['Description'].apply(lambda x: model2.encode(x, convert_to_tensor=True))\\n\\n# Convertir la columna de tensores a listas\\ndb_boardgames['tensor'] = db_boardgames['tensor'].apply(lambda x: [float(val) for val in x])\\n\\n# Exportar a JSON\\ndb_boardgames.to_json('folder/subfolder/db_boardgames.json', orient='records', lines=True)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55Sw6E5C46cL"
      },
      "source": [
        "#### Libros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16IJdVg-TzzL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "8e97380a-d830-41f1-ebaa-a4653f9fb832"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#URL de la página de Gutenberg con el top 1000 libros\\nurl = \\'https://www.gutenberg.org/browse/scores/top1000.php#books-last1\\'\\nresponse = requests.get(url)\\n\\n# Verificamos si la respuesta fue exitosa\\nif response.status_code == 200:\\n\\n    # Obtenemos los el contenido de la página, especificamente donde estan todos los titulos\\n    soup = BeautifulSoup(response.text, \\'html.parser\\')\\n    books_list = soup.select(\\'body > div.container > div > ol:nth-child(6) > li > a\\')\\n\\n    # Lista para almacenar los datos de cada libro\\n    books_data = []\\n\\n    # Iteramos sobre cada libro en la lista\\n    for book in books_list:\\n        # Extraemos el fragmento donde tiene la url para cargar la página del libro en específico\\n        book_url = f\"https://www.gutenberg.org{book[\\'href\\']}\"\\n\\n        # Obtener el contenido de la página del libro\\n        response = requests.get(book_url)\\n        soup = BeautifulSoup(response.content, \"html.parser\")\\n\\n        # Buscamos la tabla que contiene el título y la descripción\\n        table = soup.find(\"table\", {\"class\": \"bibrec\"})\\n\\n        # Extraemos el libro, buscando que el header sea Title\\n        title_row = table.find(\"th\", text=\"Title\")\\n        if title_row:\\n            title = title_row.find_next_sibling(\"td\").text.strip()\\n        else:\\n            title = \"No se encontró el título.\"\\n\\n        # Extraemos el resumen, buscando que el header sea Summary\\n        summary_row = table.find(\"th\", text=\"Summary\")\\n        if summary_row:\\n            summary = summary_row.find_next_sibling(\"td\").text.strip()\\n        else:\\n            summary = \"No se encontró el resumen.\"\\n\\n        # Cargamos el título y la descripción a una lista de diccionarios\\n        books_data.append({\\n            \\'Title\\': title,\\n            \\'Description\\': summary\\n        })\\n\\n    # Creamos un DataFrame con los datos recopilados\\n    db_books = pd.DataFrame(books_data)\\n\\nelse:\\n    print(f\"Error al acceder a la página principal. Código de estado: {response.status_code}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "\"\"\"\n",
        "#URL de la página de Gutenberg con el top 1000 libros\n",
        "url = 'https://www.gutenberg.org/browse/scores/top1000.php#books-last1'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verificamos si la respuesta fue exitosa\n",
        "if response.status_code == 200:\n",
        "\n",
        "    # Obtenemos los el contenido de la página, especificamente donde estan todos los titulos\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    books_list = soup.select('body > div.container > div > ol:nth-child(6) > li > a')\n",
        "\n",
        "    # Lista para almacenar los datos de cada libro\n",
        "    books_data = []\n",
        "\n",
        "    # Iteramos sobre cada libro en la lista\n",
        "    for book in books_list:\n",
        "        # Extraemos el fragmento donde tiene la url para cargar la página del libro en específico\n",
        "        book_url = f\"https://www.gutenberg.org{book['href']}\"\n",
        "\n",
        "        # Obtener el contenido de la página del libro\n",
        "        response = requests.get(book_url)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Buscamos la tabla que contiene el título y la descripción\n",
        "        table = soup.find(\"table\", {\"class\": \"bibrec\"})\n",
        "\n",
        "        # Extraemos el libro, buscando que el header sea Title\n",
        "        title_row = table.find(\"th\", text=\"Title\")\n",
        "        if title_row:\n",
        "            title = title_row.find_next_sibling(\"td\").text.strip()\n",
        "        else:\n",
        "            title = \"No se encontró el título.\"\n",
        "\n",
        "        # Extraemos el resumen, buscando que el header sea Summary\n",
        "        summary_row = table.find(\"th\", text=\"Summary\")\n",
        "        if summary_row:\n",
        "            summary = summary_row.find_next_sibling(\"td\").text.strip()\n",
        "        else:\n",
        "            summary = \"No se encontró el resumen.\"\n",
        "\n",
        "        # Cargamos el título y la descripción a una lista de diccionarios\n",
        "        books_data.append({\n",
        "            'Title': title,\n",
        "            'Description': summary\n",
        "        })\n",
        "\n",
        "    # Creamos un DataFrame con los datos recopilados\n",
        "    db_books = pd.DataFrame(books_data)\n",
        "\n",
        "else:\n",
        "    print(f\"Error al acceder a la página principal. Código de estado: {response.status_code}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Realizamos el embedding de BERT para cada descripción\n",
        "db_books['tensor'] = db_books['Description'].apply(lambda x: model2.encode(x, convert_to_tensor=True))\n",
        "\n",
        "# Convertir la columna de tensores a listas\n",
        "#db_books['tensor'] = db_books['tensor'].apply(lambda x: [float(val) for val in x])\n",
        "\n",
        "# Exportar a JSON\n",
        "#db_books.to_json('folder/subfolder/db_books.json', orient='records', lines=True)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jNUn3u_-qxNC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f8c0be1e-568e-4cca-c4b9-63778f0d8244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Realizamos el embedding de BERT para cada descripción\\ndb_books['tensor'] = db_books['Description'].apply(lambda x: model2.encode(x, convert_to_tensor=True))\\n\\n# Convertir la columna de tensores a listas\\n#db_books['tensor'] = db_books['tensor'].apply(lambda x: [float(val) for val in x])\\n\\n# Exportar a JSON\\n#db_books.to_json('folder/subfolder/db_books.json', orient='records', lines=True)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carga de base de datos ya tranformadas\n",
        "Películas, Libros, Juegos de mesas"
      ],
      "metadata": {
        "id": "7N8nQgTlU2pM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Descarga"
      ],
      "metadata": {
        "id": "Q5m50oH7Vg1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1lzZgGVRZ6LhiZCNT06OhqIUjcrhFCqmo\n",
        "!gdown 1A9bzLQ1SOBja8_6TiOnGADHtTAjcOaXC\n",
        "!gdown 1TaqmkromGRIauqH6KV4MdNhGtsw7QaoJ"
      ],
      "metadata": {
        "id": "o5EmziOHV3ey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34268708-31fe-4374-82e8-56b155c35350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lzZgGVRZ6LhiZCNT06OhqIUjcrhFCqmo\n",
            "To: /content/db_films.json\n",
            "100% 7.31M/7.31M [00:00<00:00, 37.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1A9bzLQ1SOBja8_6TiOnGADHtTAjcOaXC\n",
            "To: /content/db_boardgames.json\n",
            "100% 8.96M/8.96M [00:00<00:00, 59.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TaqmkromGRIauqH6KV4MdNhGtsw7QaoJ\n",
            "To: /content/db_books.json\n",
            "100% 8.12M/8.12M [00:00<00:00, 36.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Películas"
      ],
      "metadata": {
        "id": "k1iac-Z4Viw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_films = pd.read_json('/content/db_films.json', orient='records', lines=True)\n",
        "db_films['tensor'] = db_films['tensor'].apply(lambda x: torch.tensor(x))"
      ],
      "metadata": {
        "id": "gVEq5HI3U7kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Juego de Mesa"
      ],
      "metadata": {
        "id": "NIXol16fVnmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_boardgames = pd.read_json('/content/db_boardgames.json', orient='records', lines=True)\n",
        "db_boardgames['tensor'] = db_boardgames['tensor'].apply(lambda x: torch.tensor(x))"
      ],
      "metadata": {
        "id": "lrMgjQLjU7ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Libros"
      ],
      "metadata": {
        "id": "A5_KhuyAVw-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_books = pd.read_json('/content/db_books.json', orient='records', lines=True)\n",
        "db_books['tensor'] = db_books['tensor'].apply(lambda x: torch.tensor(x))"
      ],
      "metadata": {
        "id": "1WQPkjjlU7Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9mJ9105hKqI"
      },
      "source": [
        "## Creación del modelo para el estado de ánimo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rc0-Cvg6pR8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0691d8cd-8f68-47b7-a1db-dddd0a2f6ebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ES1HzfQrKSVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a22439-a8fc-4872-e13e-4146617a5c86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión Regresión Logística: 0.7472527472527473\n",
            "Reporte de clasificación Regresión Logística:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.84      0.73      0.78        37\n",
            "           0       0.69      0.71      0.70        28\n",
            "           1       0.70      0.81      0.75        26\n",
            "\n",
            "    accuracy                           0.75        91\n",
            "   macro avg       0.74      0.75      0.74        91\n",
            "weighted avg       0.76      0.75      0.75        91\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "X = db_emotion[\"frase\"].str.lower()\n",
        "y = db_emotion[\"animo\"].to_numpy() # Convertimos a numpy array\n",
        "\n",
        "# División del dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Obtenemos los embeddings de BERT para los conjuntos de entrenamiento y prueba\n",
        "X_train_vectorized = model.encode(X_train.tolist())\n",
        "X_test_vectorized = model.encode(X_test.tolist())\n",
        "\n",
        "# Creación y entrenamiento del modelo de Regresión Logística Multinomial\n",
        "modelo_LR = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
        "modelo_LR.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Evaluación del modelo de Regresión Logística\n",
        "y_pred_LR = modelo_LR.predict(X_test_vectorized)\n",
        "acc_LR = accuracy_score(y_test, y_pred_LR)\n",
        "report_LR = classification_report(y_test, y_pred_LR, zero_division=1)\n",
        "\n",
        "print(\"Precisión Regresión Logística:\", acc_LR)\n",
        "print(\"Reporte de clasificación Regresión Logística:\\n\", report_LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPrnuz_IkVi0"
      },
      "source": [
        "## Metodología de selección de un juego, pelicula o libro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TEtDwKmAK5Q"
      },
      "source": [
        "### SBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mEw0afdA9w-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69eb01da-3ac5-4a7d-f792-9b0253b437ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model2 = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFHgyXA7oPk6"
      },
      "source": [
        "## Código Principal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ys8pm8b-P59_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3c1ab6f-5e57-4f50-e307-e797bb313f9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Genial! Cuéntame sobre que quieres ver, leer o jugar: \n",
            "una cancion \n",
            "####################################################\n",
            "Top 3 peliculas que coinciden con lo que queres ver\n",
            "Búsqueda: una cancion \n",
            "\n",
            "Encontré estos resultados: \n",
            "Título: Happy Feet\n",
            "Descripción: Into the world of the Emperor Penguins, who find their soul mates through song, a penguin is born who cannot sing. But he can tap dance something fierce!\n",
            "Similitud: 0.2058\n",
            "\n",
            "Título: Mamma Mia!\n",
            "Descripción: The story of a bride-to-be trying to find her real father told using hit songs by the popular '70s group ABBA.\n",
            "Similitud: 0.1755\n",
            "\n",
            "Título: Begin Again\n",
            "Descripción: A chance encounter between a disgraced music-business executive and a young singer-songwriter new to Manhattan turns into a promising collaboration between the two talents.\n",
            "Similitud: 0.1688\n",
            "\n",
            "####################################################\n",
            "Top 3 juegos de mesa que coinciden con lo que queres ver\n",
            "Búsqueda: una cancion \n",
            "\n",
            "Encontré estos resultados: \n",
            "Título: HITSTER\n",
            "Descripción: Create an instant party with HITSTER, the music card game of the century!\n",
            "Listen to over 100 years of amazing hits, take turns arranging them in chronological order on your music timeline and create your trip down memory lane. \n",
            "The first player to collect 10 hits is crowned the HITSTER\n",
            "\n",
            "HITSTER is the perfect game for an evening with lots of laughing, singing, dancing and sharing memories. Simply open the box, scan a song card and let the music do the rest to create an instant party. So what are you waiting for?\n",
            "\n",
            "How to play\n",
            "1. Pick a music card and scan the QR code with the free Hitster app to automatically play it in Spotify\n",
            "2. Guess when the song was released by placing it in the right position of your music timeline.  \n",
            "3. Flip the music card. If correct, you keep the music card to build your timeline.\n",
            "Tip: add the Hitster tokens into the mix for an even more exciting game\n",
            "\n",
            "Features:\n",
            "- With more than 300 of the greatest hits HITSTER will create an instant party for everybody&hellip;guaranteed!\n",
            "- Everybody is invited for the HITSTER party! Super easy rules and no game set-up gets the party started in no-time. Simply scan a card with the free Hitster app and the music starts playing automatically in Spotify, isn't that cool?\n",
            "- HITSTER is the music game in which you don&rsquo;t have to be a music expert. Simply guess if a song was released before or after other songs in your music timeline. The futher you are in building your timeline the more challenging it becomes. Your chances of winning increases if you can also name the artists and song titles. \n",
            "- Are you craving to show of your music knowledge? Play HITSTER with the Pro rules or even Expert rules in which you have to know the exact year, artist and song title.\n",
            "- Fancy a less competitive trip down memory lane? Then HITSTER can also be played as one team or by yourself\n",
            "\n",
            "&mdash;description from the publisher\n",
            "\n",
            "\n",
            "Similitud: 0.1323\n",
            "\n",
            "Título: Time's Up! Title Recall!\n",
            "Descripción: Based on the popular game Time's Up!, Time's Up: Title Recall challenges players to guess the titles of books, films, songs, and more. Players try to get their teammates to guess the same set of titles over three rounds. In each round, one member of a team tries to get their teammates to guess as many titles as possible in 30 seconds. In round 1, almost any kind of clue is allowed. In round 2, no more than one word can be used in each clue (but unlimited sounds and gestures are permitted.) In round 3, no words are allowed at all.\n",
            "\n",
            "\n",
            "Similitud: 0.0725\n",
            "\n",
            "Título: Letter Jam\n",
            "Descripción: Letter Jam is a 2-6 player cooperative word game where players assist each other in composing meaningful words from letters around the table. The trick is holding the letter card so that it&rsquo;s only visible to other players and not to you.\n",
            "\n",
            "At the start of the game, each player receives a set of face-down letter cards that can be arranged to form an existing word. The setup can be prepared by using a special card scanning app, or by players selecting words for each other. Each player then puts their first card in their stand facing the other players without looking at it, and the game begins.\n",
            "\n",
            "The game is played in turns. Each turn, players simultaneously search other players&rsquo; letters to see what words they can spell out (telling the others the length of the word they can make up). The player who offers the longest word can then be chosen as the clue giver.\n",
            "\n",
            "The clue giver spells out their clue by putting numbered tokens in front of the other players. Number one goes to the player whose letter comes first in the clue, number two to the second letter etc. They can always use a wild card which can be any letter, but they cannot tell others which letter it represents.\n",
            "\n",
            "Each player with a numbered token (or tokens) in front of them then tries to figure out what their letter is. If they do, they place the card face down before revealing the next letter.  At the end of the game, players can then rearrange the cards to try to form an existing word. All players then reveal their cards to see if they were successful or not. The more players who have an existing word in front of them, the bigger their collective success.\n",
            "\n",
            "&mdash;description from the publisher\n",
            "\n",
            "\n",
            "Similitud: 0.0676\n",
            "\n",
            "####################################################\n",
            "Top 3 libros que coinciden con lo que queres ver\n",
            "Búsqueda: una cancion \n",
            "\n",
            "Encontré estos resultados: \n",
            "Título: Cowboy Songs, and Other Frontier Ballads\n",
            "Descripción: \"Cowboy Songs and Other Frontier Ballads\" by Various is a collection of folk songs and ballads that captures the essence of American cowboy culture during the late 19th to early 20th century. This anthology reflects the life, struggles, and emotions of cowboys, detailing their adventures, heartaches, and the rugged landscape of the West. The songs illustrate the camaraderie among cowboys, their love for freedom, and the challenges they faced in their profession.  The opening portion of the collection features an introduction that highlights the importance of preserving these ballads as a vital part of American folklore. It discusses the influence of the Anglo-Saxon ballad tradition in the Southwest and how these songs were created and passed down through oral recital among cowboys and other frontier folk. Notable themes include love, loss, the cowboy’s relationship with nature, and the rough lifestyle associated with cattle herding. Through the vivid imagery and emotional depth of the lyrics, readers gain insight into the unique spirit of the cowboy, who captivates through both his bravery and vulnerability. (This is an automatically generated summary.)\n",
            "Similitud: 0.1956\n",
            "\n",
            "Título: The Song Celestial; Or, Bhagavad-Gîtâ (from the Mahâbhârata)Being a discourse between Arjuna, Prince of India, and the Supreme Being under the form of Krishna\n",
            "Descripción: \"The Song Celestial; Or, Bhagavad-Gîtâ (from the Mahâbhârata)\" by Sir Edwin Arnold is a philosophical poem and spiritual discourse, likely written in the late 19th century. The text presents a dialogue between Prince Arjuna, a warrior facing a profound moral dilemma on the battlefield of Kurukshetra, and Krishna, who embodies the Supreme Being and serves as his charioteer. This discourse explores themes of duty, righteousness, and the nature of life and death, seeking to impart wisdom on both personal and cosmic levels.  The opening of the work introduces the pivotal moment in which Arjuna surveys the battle and becomes overwhelmed with grief and compassion for his relatives on both sides of the conflict. He articulates his fears and moral concerns about fighting against kinsmen, questioning the purpose and morality of war itself. As he grapples with this turmoil, Krishna responds with profound guidance, urging Arjuna to overcome his doubts and embrace his duty as a warrior, emphasizing the eternal nature of the soul and the importance of righteous action. This sets the stage for the philosophical journey that will unfold throughout the text. (This is an automatically generated summary.)\n",
            "Similitud: 0.1712\n",
            "\n",
            "Título: Lyrical Ballads, With a Few Other Poems (1798)\n",
            "Descripción: \"Lyrical Ballads, With a Few Other Poems\" by Wordsworth and Coleridge is a collection of poetry from the late 18th century that marks a significant turning point in English literature, being a cornerstone of the Romantic movement. This work primarily explores the lives, emotions, and experiences of common people, employing language accessible to the middle and lower classes, which diverges from the ornate style of earlier poetry. The poets aim to provoke a deeper appreciation of nature and the human condition through their verses.  The opening of the collection introduces the intention behind the poems and the methodologies employed by the authors. They express a desire to experiment with conversational language while depicting human emotions in a naturalistic manner. The beginning discusses various poems within the collection, mentioning characters such as the titular Ancient Mariner, who shares harrowing tales of his maritime experiences, and Goody Blake, a poor woman whose actions lead to mysterious consequences. This initial section sets the stage for readers to encounter various narratives that reflect the intertwined relationships between humanity and nature, as well as the complexities of life itself. (This is an automatically generated summary.)\n",
            "Similitud: 0.1511\n",
            "\n",
            "####################################################\n"
          ]
        }
      ],
      "source": [
        "start()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}